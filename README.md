
# Hands-on Docker

## Step 3 : Writing your own image

Using a ready-to-use image was nice, but we didn't really learn how to write a `Dockerfile`. So, let's try again, but this time using a more basic base image : we will use the [ubuntu image](https://registry.hub.docker.com/_/ubuntu/) as a base image.

So, starting from a basic Ubuntu, we will need to :

* install python and pip
* copy our application files to the image, in the right directory
* run a `pip install` to get the application's dependencies in the image
* define the port used by the application
* define the command used to start the application

Let's have a look at our new `Dockerfile`, step by step :

* declare the base image (we will use the latest Ubuntu LTS)

  ```
  FROM ubuntu:18.04
  ```
* install python, pip and requirements:
  
  ```
  RUN apt-get update && apt-get install -y \
    python3 python3-pip \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*
  ```
  a few notes here :
  * we chain multiple commands in a single instruction to avoid creating too much intermediate images
  * we clean up at the end of the instruction in order to save some space in the final image - see [this link](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/) for more information
* choose a directory for our application, create it, and use it as the current directory

  ```
  RUN mkdir -p /code
  WORKDIR /code
  ```
  You can read more about the [`WORKDIR` instruction](https://docs.docker.com/reference/builder/#workdir) used to define a directory as the current directory
* copy our application files to the image, and run `pip install`

  ```
  COPY app/requirements.txt /code
  RUN pip install --upgrade pip && pip install -r requirements.txt
  COPY app /code
  ```
  here again, a few notes are required :
  * we could have used a basic
    ```
    COPY . /code
    RUN pip3 install --upgrade pip && pip install -r requirements.txt
    ```
    to copy the current directory (`.`) from your machine to the `/code` directory in the image, and then simply run `pip install` (in the `/code` directory of the image).
The problem with this solution, is that the [`COPY` instruction](https://docs.docker.com/reference/builder/#copy) will clear the cache if a file has been changed. So, each time you change a file, docker will re-upload everything and re-run `pip install`. And we need to execute `pip install` only if we change the `requirements.txt` file (otherwise we might end up with different dependencies versions, and so on).
  * so the solution to this problem is to run `pip install` BEFORE copying all the files. But we still need the `requirements.txt` **BEFORE** running `pip install`. So, we copy the `requirements.txt` file, then run `pip install`, and then copy all the other files.
* declare that the application listens on the port 8080
  ```
  EXPOSE 8080
  ```
* define the command used to start the application (`python3 app.py`), using the [`CMD` instruction](https://docs.docker.com/reference/builder/#cmd)

  ```
  CMD [ "python3", "app.py" ]
  ```

You can now build the image, using the following command :

```
docker build -t handsondocker:ubuntu .
```
Notice that the tag is now `handsondocker:ubuntu`. If you don't provide a tag, docker will use by default `latest` (so our previous image was `handsondocker:latest`). To differentiate between the 2 images, we will tag this one `ubuntu` to indicate that it is built from an ubuntu base image.

If you run a new container, based on this new image, with

```
docker run --rm -it -p 8080:80 handsondocker:ubuntu
```
you should be able to play with the application the same way you did with the previous container.

You can now use `Ctrl+C` to stop the container.

### Private Docker Image repositories

Until now, your were using public images, but now, let's use private images (from a private Docker registry - you can find the official documentation at https://docs.docker.com/engine/reference/commandline/login/):

  * Adding credentials :

```
docker login [DOCKER_IMAGE_PRIVATE_REPOSITORY_URL] -u [DOCKER_IMAGE_PRIVATE_REPOSITORY_USER] -p [DOCKER_IMAGE_PRIVATE_REPOSITORY_PASSWORD]
```
   
  * From now on, you will be able to consume images (if your "user" has "read" privileges) from the private Image repository

As you can see, all our images are stored locally, but we need to share them with the team so, let's push the image to the docker registry. Just type the command below:

```
docker push [IMAGE_NAME]
```

### Persistence

By default all data generated by your application is stored wihtin the container, that means, once your container is stopped/destroyed, the data is lost.

Docker provides a very good mechanism to persist data (such as database content). This mechanism is driven by [Volumes](https://docs.docker.com/engine/tutorials/dockervolumes/)

  * Adding a volume to your container:

```
docker run --rm -it -p 3000:3000 -v /opt/data/docker_vol/handson:/var/data/logs handsondocker:ubuntu
```

  * the `-v` flag mounts the host file system to the container file system. Next time you run the container, the data stored by your application will be there. NOTE: in this example, the host directory /opt/data/docker_vol/handson is mounted at /var/data/logs inside the container

### Configuring containers
According to Docker best practices, Docker images have to be ready to be adapted in order to be executed in different contexts (different environments), and kwnowing that a Docker image is inmutable, you have to able to provide this capability to your image.

Docker provides this capability by injecting **ENVIRONMENT VARIABLES**, as a consequence, your source code has to be ready to read those variables.

Below the way to send **ENVIRONMENT VARIABLES** to your container:

```
docker run -d -p 80 --name myimage -e ENV_VAR1="env_var1_value" -e ENV_VAR2="env_var2_value" myimage
```

### Managing Docker Containers

At any time you can access to see which containers are in your computer typing the command below:

```
docker ps -a
```

* the `ps` shows the container status, adding `-a` flag will show you all containers (no matter if they are running or stopped)

### Clean up images and containers

As you can see, working with Docker is so easy, but there is just one drawback: HDD Space.

Images and Containers take space at your computer. Now you will see a set of useful commands to clean your storage:

  * Remove exited(stopped) containers:

```
docker rm $(docker ps --filter "status=exited" -q)
```


  * Remove unused orphan images (`dangling=true` flag filters orphan images)

```
docker rmi $(docker images --filter dangling=true -q) 
```

You can now move to the [Step4](https://github.com/peppelin/hands-on-docker/tree/step4)
